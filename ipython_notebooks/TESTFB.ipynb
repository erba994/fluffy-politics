{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "creator": "admin",
    "createdOn": 1634026539876,
    "tags": [],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "Populating the interactive namespace from numpy and matplotlib\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 6,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd ..\n%cd fbcrawl-master\n!ls"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/erba994/dataikudir/jupyter-run/dku-workdirs/POLITICAFB\nfacebook-post-scraper  fbcrawl-master  TESTFB3f97ca69\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 15,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!scrapy crawl events -a email\u003d\"lorenzotosi1206@gmail.com\" -a password\u003d\"Cycling1294\" -a page\u003d\"https://mbasic.facebook.com/groups/675823305849812/\" -o \"pagnagna.csv\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/erba994/dataikudir/jupyter-run/dku-workdirs/POLITICAFB/fbcrawl-master/fbcrawl/items.py:572: ScrapyDeprecationWarning: scrapy.loader.processors.Join is deprecated, instantiate itemloaders.processors.Join instead.\n  output_processor\u003dJoin(separator\u003du\u0027\u0027)\n/home/erba994/dataikudir/jupyter-run/dku-workdirs/POLITICAFB/fbcrawl-master/fbcrawl/items.py:615: ScrapyDeprecationWarning: scrapy.loader.processors.Join is deprecated, instantiate itemloaders.processors.Join instead.\n  output_processor\u003dJoin(separator\u003du\u0027\u0027)\n2021-10-12 14:00:13 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: fbcrawl)\n2021-10-12 14:00:13 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.2 (default, Feb 28 2021, 17:03:44) - [GCC 10.2.1 20210110], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 35.0.0, Platform Linux-5.10.60.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\n2021-10-12 14:00:13 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n2021-10-12 14:00:13 [scrapy.crawler] INFO: Overridden settings:\n{\u0027BOT_NAME\u0027: \u0027fbcrawl\u0027,\n \u0027CONCURRENT_REQUESTS\u0027: 1,\n \u0027DOWNLOAD_DELAY\u0027: 3,\n \u0027DUPEFILTER_CLASS\u0027: \u0027scrapy.dupefilters.BaseDupeFilter\u0027,\n \u0027FEED_EXPORT_ENCODING\u0027: \u0027utf-8\u0027,\n \u0027FEED_EXPORT_FIELDS\u0027: [\u0027name\u0027,\n                        \u0027where\u0027,\n                        \u0027location\u0027,\n                        \u0027photo\u0027,\n                        \u0027start_date\u0027,\n                        \u0027end_date\u0027,\n                        \u0027description\u0027],\n \u0027HTTPCACHE_STORAGE\u0027: \u0027scrapy_splash.SplashAwareFSCacheStorage\u0027,\n \u0027NEWSPIDER_MODULE\u0027: \u0027fbcrawl.spiders\u0027,\n \u0027SPIDER_MODULES\u0027: [\u0027fbcrawl.spiders\u0027],\n \u0027URLLENGTH_LIMIT\u0027: 99999,\n \u0027USER_AGENT\u0027: \u0027NokiaC3-00/5.0 (07.20) Profile/MIDP-2.1 Configuration/CLDC-1.1 \u0027\n               \u0027Mozilla/5.0 AppleWebKit/420+ (KHTML, like Gecko) Safari/420+\u0027}\n2021-10-12 14:00:13 [scrapy.extensions.telnet] INFO: Telnet Password: 52d2880c2153d7d8\n2021-10-12 14:00:13 [scrapy.middleware] INFO: Enabled extensions:\n[\u0027scrapy.extensions.corestats.CoreStats\u0027,\n \u0027scrapy.extensions.telnet.TelnetConsole\u0027,\n \u0027scrapy.extensions.memusage.MemoryUsage\u0027,\n \u0027scrapy.extensions.feedexport.FeedExporter\u0027,\n \u0027scrapy.extensions.logstats.LogStats\u0027]\n2021-10-12 14:00:13 [events] INFO: Email and password provided, will be used to log in\n2021-10-12 14:00:13 [events] INFO: Date attribute not provided, scraping date set to 2004-02-04 (fb launch date)\n2021-10-12 14:00:13 [events] INFO: Language attribute not provided, fbcrawl will try to guess it from the fb interface\n2021-10-12 14:00:13 [events] INFO: To specify, add the lang parameter: scrapy fb -a lang\u003d\"LANGUAGE\"\n2021-10-12 14:00:13 [events] INFO: Currently choices for \"LANGUAGE\" are: \"en\", \"es\", \"fr\", \"it\", \"pt\"\n2021-10-12 14:00:14 [scrapy.core.engine] INFO: Spider opened\n2021-10-12 14:00:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2021-10-12 14:00:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n2021-10-12 14:00:14 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (meta refresh) to \u003cGET https://mbasic.facebook.com/cookie/consent-page/?next_uri\u003dhttps%3A%2F%2Fmbasic.facebook.com%2F\u0026refsrc\u003ddeprecated\u0026_rdr\u003e from \u003cGET https://mbasic.facebook.com\u003e\n2021-10-12 14:00:17 [scrapy.core.engine] DEBUG: Crawled (200) \u003cGET https://mbasic.facebook.com/cookie/consent-page/?next_uri\u003dhttps%3A%2F%2Fmbasic.facebook.com%2F\u0026refsrc\u003ddeprecated\u0026_rdr\u003e (referer: None)\n2021-10-12 14:00:17 [scrapy.core.scraper] ERROR: Spider error processing \u003cGET https://mbasic.facebook.com/cookie/consent-page/?next_uri\u003dhttps%3A%2F%2Fmbasic.facebook.com%2F\u0026refsrc\u003ddeprecated\u0026_rdr\u003e (referer: None)\nTraceback (most recent call last):\n  File \"/home/erba994/.local/lib/python3.9/site-packages/twisted/internet/defer.py\", line 858, in _runCallbacks\n    current.result \u003d callback(  # type: ignore[misc]\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/spiders/__init__.py\", line 90, in _parse\n    return self.parse(response, **kwargs)\n  File \"/home/erba994/dataikudir/jupyter-run/dku-workdirs/POLITICAFB/fbcrawl-master/fbcrawl/spiders/fbcrawl.py\", line 90, in parse\n    return FormRequest.from_response(\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/http/request/form.py\", line 48, in from_response\n    form \u003d _get_form(response, formname, formid, formnumber, formxpath)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/http/request/form.py\", line 106, in _get_form\n    raise ValueError(f\u0027No \u003cform\u003e element found with {formxpath}\u0027)\nValueError: No \u003cform\u003e element found with //form[contains(@action, \"login\")]\n2021-10-12 14:00:18 [scrapy.core.engine] INFO: Closing spider (finished)\n2021-10-12 14:00:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{\u0027downloader/request_bytes\u0027: 705,\n \u0027downloader/request_count\u0027: 2,\n \u0027downloader/request_method_count/GET\u0027: 2,\n \u0027downloader/response_bytes\u0027: 3417,\n \u0027downloader/response_count\u0027: 2,\n \u0027downloader/response_status_count/200\u0027: 2,\n \u0027elapsed_time_seconds\u0027: 3.991721,\n \u0027finish_reason\u0027: \u0027finished\u0027,\n \u0027finish_time\u0027: datetime.datetime(2021, 10, 12, 12, 0, 18, 47850),\n \u0027httpcompression/response_bytes\u0027: 4621,\n \u0027httpcompression/response_count\u0027: 2,\n \u0027log_count/DEBUG\u0027: 2,\n \u0027log_count/ERROR\u0027: 1,\n \u0027log_count/INFO\u0027: 12,\n \u0027memusage/max\u0027: 58441728,\n \u0027memusage/startup\u0027: 58441728,\n \u0027response_received_count\u0027: 1,\n \u0027scheduler/dequeued\u0027: 2,\n \u0027scheduler/dequeued/memory\u0027: 2,\n \u0027scheduler/enqueued\u0027: 2,\n \u0027scheduler/enqueued/memory\u0027: 2,\n \u0027spider_exceptions/ValueError\u0027: 1,\n \u0027start_time\u0027: datetime.datetime(2021, 10, 12, 12, 0, 14, 56129)}\n2021-10-12 14:00:18 [scrapy.core.engine] INFO: Spider closed (finished)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from twisted.internet import reactor\nimport scrapy\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\n\nconfigure_logging({\u0027LOG_FORMAT\u0027: \u0027%(levelname)s: %(message)s\u0027})\nrunner \u003d CrawlerRunner()\n\nd \u003d runner.crawl(MySpider)\nd.addBoth(lambda _: reactor.stop())\nreactor.run() # the script will block here until the crawling is finished\nSee also"
      ],
      "outputs": []
    }
  ]
}