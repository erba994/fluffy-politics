{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "creator": "admin",
    "createdOn": 1634026539876,
    "tags": [],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "Populating the interactive namespace from numpy and matplotlib\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 6,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd ..\n%cd fbcrawl-master\n!ls"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/erba994/dataikudir/jupyter-run/dku-workdirs/POLITICAFB\nfacebook-post-scraper  fbcrawl-master  TESTFB3f97ca69\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 14,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!scrapy crawl events -a email\u003d\"lorenzotosi1206@gmail.com\" -a password\u003d\"Cycling1294\" -a page\u003d\"https://mbasic.facebook.com/groups/675823305849812/\" -o \"pagnagna.csv\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/erba994/dataikudir/jupyter-run/dku-workdirs/POLITICAFB/fbcrawl-master/fbcrawl/items.py:572: ScrapyDeprecationWarning: scrapy.loader.processors.Join is deprecated, instantiate itemloaders.processors.Join instead.\n  output_processor\u003dJoin(separator\u003du\u0027\u0027)\n/home/erba994/dataikudir/jupyter-run/dku-workdirs/POLITICAFB/fbcrawl-master/fbcrawl/items.py:615: ScrapyDeprecationWarning: scrapy.loader.processors.Join is deprecated, instantiate itemloaders.processors.Join instead.\n  output_processor\u003dJoin(separator\u003du\u0027\u0027)\n2021-10-12 13:57:38 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: fbcrawl)\n2021-10-12 13:57:38 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.9.2 (default, Feb 28 2021, 17:03:44) - [GCC 10.2.1 20210110], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 35.0.0, Platform Linux-5.10.60.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\n2021-10-12 13:57:38 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n2021-10-12 13:57:38 [scrapy.crawler] INFO: Overridden settings:\n{\u0027BOT_NAME\u0027: \u0027fbcrawl\u0027,\n \u0027CONCURRENT_REQUESTS\u0027: 1,\n \u0027DOWNLOAD_DELAY\u0027: 3,\n \u0027DUPEFILTER_CLASS\u0027: \u0027scrapy.dupefilters.BaseDupeFilter\u0027,\n \u0027FEED_EXPORT_ENCODING\u0027: \u0027utf-8\u0027,\n \u0027FEED_EXPORT_FIELDS\u0027: [\u0027name\u0027,\n                        \u0027where\u0027,\n                        \u0027location\u0027,\n                        \u0027photo\u0027,\n                        \u0027start_date\u0027,\n                        \u0027end_date\u0027,\n                        \u0027description\u0027],\n \u0027HTTPCACHE_STORAGE\u0027: \u0027scrapy_splash.SplashAwareFSCacheStorage\u0027,\n \u0027NEWSPIDER_MODULE\u0027: \u0027fbcrawl.spiders\u0027,\n \u0027SPIDER_MODULES\u0027: [\u0027fbcrawl.spiders\u0027],\n \u0027URLLENGTH_LIMIT\u0027: 99999,\n \u0027USER_AGENT\u0027: \u0027NokiaC3-00/5.0 (07.20) Profile/MIDP-2.1 Configuration/CLDC-1.1 \u0027\n               \u0027Mozilla/5.0 AppleWebKit/420+ (KHTML, like Gecko) Safari/420+\u0027}\n2021-10-12 13:57:38 [scrapy.extensions.telnet] INFO: Telnet Password: 1576ab9dea1a3e0c\n2021-10-12 13:57:38 [scrapy.middleware] INFO: Enabled extensions:\n[\u0027scrapy.extensions.corestats.CoreStats\u0027,\n \u0027scrapy.extensions.telnet.TelnetConsole\u0027,\n \u0027scrapy.extensions.memusage.MemoryUsage\u0027,\n \u0027scrapy.extensions.feedexport.FeedExporter\u0027,\n \u0027scrapy.extensions.logstats.LogStats\u0027]\n2021-10-12 13:57:38 [events] INFO: Email and password provided, will be used to log in\n2021-10-12 13:57:38 [events] INFO: Date attribute not provided, scraping date set to 2004-02-04 (fb launch date)\n2021-10-12 13:57:38 [events] INFO: Language attribute not provided, fbcrawl will try to guess it from the fb interface\n2021-10-12 13:57:38 [events] INFO: To specify, add the lang parameter: scrapy fb -a lang\u003d\"LANGUAGE\"\n2021-10-12 13:57:38 [events] INFO: Currently choices for \"LANGUAGE\" are: \"en\", \"es\", \"fr\", \"it\", \"pt\"\nUnhandled error in Deferred:\n2021-10-12 13:57:38 [twisted] CRITICAL: Unhandled error in Deferred:\n\nTraceback (most recent call last):\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/crawler.py\", line 192, in crawl\n    return self._crawl(crawler, *args, **kwargs)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/crawler.py\", line 196, in _crawl\n    d \u003d crawler.crawl(*args, **kwargs)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/twisted/internet/defer.py\", line 1909, in unwindGenerator\n    return _cancellableInlineCallbacks(gen)  # type: ignore[unreachable]\n  File \"/home/erba994/.local/lib/python3.9/site-packages/twisted/internet/defer.py\", line 1816, in _cancellableInlineCallbacks\n    _inlineCallbacks(None, gen, status)\n--- \u003cexception caught here\u003e ---\n  File \"/home/erba994/.local/lib/python3.9/site-packages/twisted/internet/defer.py\", line 1661, in _inlineCallbacks\n    result \u003d current_context.run(gen.send, result)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/crawler.py\", line 87, in crawl\n    self.engine \u003d self._create_engine()\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/crawler.py\", line 101, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/core/engine.py\", line 69, in __init__\n    self.downloader \u003d downloader_cls(crawler)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/core/downloader/__init__.py\", line 83, in __init__\n    self.middleware \u003d DownloaderMiddlewareManager.from_crawler(crawler)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/middleware.py\", line 53, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/middleware.py\", line 34, in from_settings\n    mwcls \u003d load_object(clspath)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/utils/misc.py\", line 61, in load_object\n    mod \u003d import_module(module)\n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"\u003cfrozen importlib._bootstrap\u003e\", line 1030, in _gcd_import\n    \n  File \"\u003cfrozen importlib._bootstrap\u003e\", line 1007, in _find_and_load\n    \n  File \"\u003cfrozen importlib._bootstrap\u003e\", line 984, in _find_and_load_unlocked\n    \nbuiltins.ModuleNotFoundError: No module named \u0027scrapy_splash\u0027\n\n2021-10-12 13:57:38 [twisted] CRITICAL: \nTraceback (most recent call last):\n  File \"/home/erba994/.local/lib/python3.9/site-packages/twisted/internet/defer.py\", line 1661, in _inlineCallbacks\n    result \u003d current_context.run(gen.send, result)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/crawler.py\", line 87, in crawl\n    self.engine \u003d self._create_engine()\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/crawler.py\", line 101, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/core/engine.py\", line 69, in __init__\n    self.downloader \u003d downloader_cls(crawler)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/core/downloader/__init__.py\", line 83, in __init__\n    self.middleware \u003d DownloaderMiddlewareManager.from_crawler(crawler)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/middleware.py\", line 53, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/middleware.py\", line 34, in from_settings\n    mwcls \u003d load_object(clspath)\n  File \"/home/erba994/.local/lib/python3.9/site-packages/scrapy/utils/misc.py\", line 61, in load_object\n    mod \u003d import_module(module)\n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"\u003cfrozen importlib._bootstrap\u003e\", line 1030, in _gcd_import\n  File \"\u003cfrozen importlib._bootstrap\u003e\", line 1007, in _find_and_load\n  File \"\u003cfrozen importlib._bootstrap\u003e\", line 984, in _find_and_load_unlocked\nModuleNotFoundError: No module named \u0027scrapy_splash\u0027\n",
          "name": "stdout"
        }
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from twisted.internet import reactor\nimport scrapy\nfrom scrapy.crawler import CrawlerRunner\nfrom scrapy.utils.log import configure_logging\n\nconfigure_logging({\u0027LOG_FORMAT\u0027: \u0027%(levelname)s: %(message)s\u0027})\nrunner \u003d CrawlerRunner()\n\nd \u003d runner.crawl(MySpider)\nd.addBoth(lambda _: reactor.stop())\nreactor.run() # the script will block here until the crawling is finished\nSee also"
      ],
      "outputs": []
    }
  ]
}